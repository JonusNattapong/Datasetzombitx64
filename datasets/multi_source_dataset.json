[
  {
    "source": "youtube",
    "query": "https://www.youtube.com/watch?v=OTkq4OsG_Yc",
    "task": "summarize",
    "prompt": "Summarize the video content",
    "content": "Title: Introduction to Poker Theory\nDescription: MIT 15.S50 Poker Theory and Analysis, IAP 2015\nView the complete course: http://ocw.mit.edu/15-S50IAP15\nInstructor: Kevin Desmond\n\nAn overview of the course requirements, expectations, software used for tournaments, advanced techniques, and some basics tools and concepts for the class are discussed in this lecture.\n\nLicense: Creative Commons BY-NC-SA\nMore information at http://ocw.mit.edu/terms\nMore courses at http://ocw.mit.edu",
    "result": {
      "result": "**Summary of the Video \"Introduction to Poker Theory\":**\n\n- **Course**: MIT 15.S50 Poker Theory and Analysis (IAP 2015)\n- **Instructor**: Kevin Desmond\n- **Content**:\n  - Overview of course requirements and expectations.\n  - Introduction to the software used for poker tournaments.\n  - Discussion of advanced poker techniques.\n  - Presentation of basic tools and concepts necessary for the class.\n- **Additional Information**:\n  - Full course available on MIT OpenCourseWare.\n  - License: Creative Commons BY-NC-SA.\n  - More details and courses available at MIT OpenCourseWare website.\n\nThis introductory lecture sets the foundation for the course by outlining what students need to know to succeed and providing essential resources and concepts."
    },
    "processed_by": "mistral"
  },
  {
    "source": "google",
    "query": "Poker Theory",
    "task": "analyze",
    "prompt": "Analyze the sentiment and key points",
    "content": "Title: Poker Theory\nLink: https://www.reddit.com/r/Poker_Theory/\nSnippet: Nov 28, 2016 ... When you have a good hand, raise 4x BB + 1 BB for every person in the hand. If it's a 3 bet then AT LEAST bet 3.5x but if you can get away with ...\n\nTitle: Game Theory Optimal (GTO) Texas Holdem Poker Theory ...\nLink: https://blogs.cornell.edu/info2040/2021/11/03/game-theory-optimal-gto-texas-holdem-poker-theory/\nSnippet: Nov 3, 2021 ... In general, the optimal theory dictates that the player is always making the decision that returns the most profit in the long run. For example, ...\n\nTitle: Old school vs new school and the fundamental theorem of poker | by ...\nLink: https://mikefowlds.medium.com/old-school-vs-new-school-and-the-fundamental-theorem-of-poker-3c91e865b281\nSnippet: Feb 14, 2024 ... David Sklansky, in his book 'the theory of poker' (new edition 1999) stated that Every time you play a hand differently from the way you would ...\n\nTitle: Modern poker theory complicated or no? : r/Poker_Theory\nLink: https://www.reddit.com/r/Poker_Theory/comments/1cfrj9d/modern_poker_theory_complicated_or_no/\nSnippet: Apr 29, 2024 ... The overarching theory of stuff is pretty simple. In any given position, you need to have bluffs and value hands. If you don't have enough ...\n\nTitle: Poker strategy - Wikipedia\nLink: https://en.wikipedia.org/wiki/Poker_strategy\nSnippet: Reasons to raise · To get more money in the pot when a player has the best hand: · To drive out opponents when a player has the best hand: · To bluff · To semi- ...\n\n",
    "result": {
      "result": "### Sentiment Analysis\n\nThe sentiment of the provided text snippets is generally neutral to positive, with a focus on informative and analytical content related to poker theory and strategy. Here's a breakdown of the sentiment for each snippet:\n\n1. **Poker Theory (Reddit)**\n   - Sentiment: Neutral to Positive\n   - The snippet provides practical advice on raising bets in poker, which is informative and neutral in tone.\n\n2. **Game Theory Optimal (GTO) Texas Holdem Poker Theory (Cornell Blog)**\n   - Sentiment: Neutral to Positive\n   - The text explains the concept of optimal decision-making in poker, which is informative and neutral in tone.\n\n3. **Old School vs New School and the Fundamental Theorem of Poker (Medium)**\n   - Sentiment: Neutral to Positive\n   - The snippet references a classic poker theory book and discusses the fundamental theorem of poker, which is analytical and neutral in tone.\n\n4. **Modern Poker Theory Complicated or No? (Reddit)**\n   - Sentiment: Neutral to Positive\n   - The text discusses the simplicity of modern poker theory, which is informative and neutral in tone.\n\n5. **Poker Strategy (Wikipedia)**\n   - Sentiment: Neutral to Positive\n   - The snippet lists reasons for raising in poker, which is informative and neutral in tone.\n\n### Key Points\n\n1. **Poker Theory (Reddit)**\n   - **Key Points:**\n     - When holding a good hand, raise 4x the big blind (BB) plus 1 BB for each additional player.\n     - For a 3-bet situation, bet at least 3.5x the BB.\n\n2. **Game Theory Optimal (GTO) Texas Holdem Poker Theory (Cornell Blog)**\n   - **Key Points:**\n     - Optimal poker theory involves making decisions that maximize long-term profit.\n     - Example-driven approach to explain GTO strategies.\n\n3. **Old School vs New School and the Fundamental Theorem of Poker (Medium)**\n   - **Key Points:**\n     - Discussion of David Sklansky's theory of poker.\n     - Emphasis on the fundamental theorem of poker, which suggests playing each hand in a way that maximizes expected value.\n\n4. **Modern Poker Theory Complicated or No? (Reddit)**\n   - **Key Points:**\n     - Modern poker theory is described as relatively simple.\n     - Importance of having a balance of bluffs and value hands in any given position.\n\n5. **Poker Strategy (Wikipedia)**\n   - **Key Points:**\n     - Reasons for raising in poker include getting more money in the pot when having the best hand, driving out opponents, bluffing, and semi-bluffing.\n     - General overview of strategic considerations in poker.\n\n### Summary\n\nThe snippets collectively provide a comprehensive overview of various aspects of poker theory and strategy. The sentiment is largely neutral to positive, focusing on providing informative and analytical insights. Key points revolve around practical betting strategies, the application of game theory, the evolution of poker theory, and the reasons behind different poker moves."
    },
    "processed_by": "mistral"
  },
  {
    "source": "pdf",
    "query": "https://arxiv.org/pdf/2502.19634",
    "task": "extract",
    "prompt": "Extract key findings",
    "content": "MedVLM-R1: Incentivizing Medical Reasoning\nCapability of Vision-Language Models (VLMs)\nvia Reinforcement Learning\nJiazhen Pan1,2∗, Che Liu3∗, Junde Wu2, Fenglin Liu2, Jiayun Zhu2, Hongwei\nBran Li4, Chen Chen5,6, Cheng Ouyang2,6†, Daniel Rueckert1,6†\n1Chair for AI in Healthcare and Medicine, Technical University of Munich (TUM)\nand TUM University Hospital, Germany\n2Department of Engineering Science, University of Oxford, UK\n3Data Science Institute, Imperial College London, UK\n4Massachusetts General Hospital, Harvard Medical School, USA\n5School of Computer Science, University of Sheffield, UK\n6Department of Computing, Imperial College London, UK\nAbstract. Reasoning is a critical frontier for advancing medical im-\nage analysis, where transparency and trustworthiness play a central role\nin both clinician trust and regulatory approval. Although Medical Vi-\nsual Language Models (VLMs) show promise for radiological tasks, most\nexisting VLMs merely produce final answers without revealing the un-\nderlying reasoning. To address this gap, we introduce MedVLM-R1, a\nmedical VLM that explicitly generates natural language reasoning to\nenhance transparency and trustworthiness. Instead of relying on super-\nvised fine-tuning (SFT), which often suffers from overfitting to training\ndistributions and fails to foster genuine reasoning, MedVLM-R1 employs\na reinforcement learning framework that incentivizes the model to dis-\ncover human-interpretable reasoning paths without using any reasoning\nreferences. Despite limited training data (600 visual question answering\nsamples)andmodelparameters(2B),MedVLM-R1boostsaccuracyfrom\n55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperform-\ning larger models trained on over a million samples. It also demonstrates\nrobust domain generalization under out-of-distribution tasks. By unify-\ning medical image analysis with explicit reasoning, MedVLM-R1 marks a\npivotal step toward trustworthy and interpretable AI in clinical practice.\nKeywords: Medical reasoning ·Reinforcement learning ·VLMs\n1 Introduction\nRadiological images are fundamental to modern healthcare, with over 8 bil-\nlion scans performed annually [2]. As diagnostic demand grows, the demand for\n∗Equal contribution\n†Equal advicearXiv:2502.19634v1  [cs.CV]  26 Feb 20252 Pan, Liu et al.\nefficient AI-driven interpretation becomes increasingly acute. Medical Vision-\nLanguage Models (VLMs), developed for radiological visual question answering\n(VQA) in MRI, CT and X-ray images, offer substantial promise in assisting clin-\nicians/patients. Recent advances in general-purpose LLMs/VLMs (e.g., GPT-\n4o [16], Claude-3.7 Sonnet [3]) highlight sophisticated reasoning capabilities.\nHowever, the medical domain places an especially high premium on explainable\ndecision-making: both clinicians/patients need to understand not just whatcon-\nclusion was reached, but also why. Existing medical VLMs often provide only\nfinalanswersor“quasi-explanations” derivedfrompre-trainingpatternmatching,\nwhich do not necessarily reflect genuine, step-by-step reasoning. Consequently,\nensuringinterpretabilityandtrustworthinessremainsanurgentchallengeinreal-\nworld clinical settings.\nWe argue that the limited reasoning capability for existing medical VLM\nis primarily due to the inherent drawbacks of Supervised Fine-Tuning (SFT)\n[1, 25, 9] which is the most common strategy for adapting large foundation\nmodels for specialized medical tasks [13, 21, 6]. Despite its simplicity, SFT faces\ntwocriticalchallenges:1)Anover-relianceonfinal-answersupervisionoftenleads\nto overfitting, shortcut learning, and weaker performance on out-of-distribution\n(OOD) data – an issue particularly consequential in high-stake medical scenarios\n[11]. 2) Direct supervision with only final answers provides minimal incentive for\ncultivating reasoning abilities within VLMs. A possible mitigation is distilling a\nmore capable teacher model’s chain-of-thought (CoT) reasoning for SFT [31, 20].\nHowever, constructing high-quality CoT data is prohibitively expensive to scale\nin specialized domains like healthcare. As a result, current medical VLMs that\nrely on SFT often fall short of delivering transparent explanations and robust\ngeneralizations when confronted with unfamiliar data.\nIn contrast, Reinforcement Learning (RL) [26] offers a compelling alterna-\ntive for cultivating emergent reasoning by rewarding models for discovering their\nown logical steps rather than memorizing final answers or copying teacher CoT\nrationales. Indeed, a recent work SFT Memorizes, RL Generalizes [11] con-\nfirms that RL-trained models often display superior generalization compared to\ntheir SFT counterparts. However, conventional RL pipelines typically depend on\nauxiliary neural reward models, requiring substantial resources to continuously\nupdate both policy and reward models [35, 24]. A promising alternative, group\nrelative policy optimization (GRPO) [27], eliminates the need for neural reward\nmodels by employing a rule-based group-relative advantage strategy (see sec. 3\nfor more details). This approach has demonstrated advanced reasoning, fostering\ncapabilities while reducing computational demands in DeepSeek-R1 [12]. Despite\nits potential benefits for resource- and data-constrained domains like healthcare,\nGRPO remains largely unexplored in medical contexts.\nIn this work, we introduce MedVLM-R1 , the first medical VLM capable of\ngenerating answers with explicit reasoning by training with GRPO for radiology\nVQA tasks. Our contributions are as follows:MedVLM-R1 3\n1.Medical VLM with Explicit Reasoning : We introduce MedVLM-R1,\nthe first lightweight medical VLM capable of generating explicit reasoning\nalongside the final answer, rather than providing only the final answer.\n2.Emerging Reasoning Without Explicit Supervision : Unlike tradi-\ntionalSFTmethodsthatrequiredatawithcomplexreasoningsteps,MedVLM-\nR1 is trained using GRPO with datasets containing only final answers,\ndemonstrating emergent reasoning capabilities without explicit supervision.\n3.Superior Generalization and Efficiency : MedVLM-R1 achieves robust\ngeneralization to out-of-distribution data (e.g. MRI →CT/X-ray) and out-\nperforms larger models like Qwen2VL-72B and Huatuo-GPT-Vision-7B, de-\nspite being a compact 2B-parameter model trained on just 600 samples.\n2 Related Work\nMedical VLMs and Their Limitations. The rise of large-scale VLMs has\nspurred numerous domain-specific adaptations for healthcare, with systems such\nas LLaVA-Med [19] and HuatuoGPT-Vision [7] achieving impressive results in\nradiology VQA and related diagnostic tasks. Despite these advancements, using\nSFT on final-answer labels remains the dominant strategy for tailoring large\nmodels to medical domains [34, 6, 33, 5]. This approach generally requires sub-\nstantial amounts of high-quality image-text data (ranging from 660k [19] to 32M\nsamples [32]) which is costly to curate and often hampered by noise/privacy con-\ncerns. Moreover, the reliance on final-answer supervision provides limited scope\nfor exposing a model’s intermediate reasoning—an important factor in building\nclinicians’ trust. In addition, SFT-based models often overfit to narrow training\ndistributions, leading to weaker generalization on OOD clinical scenarios.\nReinforcement Learning for Enhanced Reasoning. To mitigate SFT’s\nlimitations, RL [29, 10, 35, 24, 17] has emerged as a compelling alternative for\nimproving model interpretability and robustness. Classic RL methods, such as\nproximalpolicyoptimization(PPO)[26],havebeenwidelyadoptedintext-based\nlearning ( e.g.,policy shaping for LLMs) and can reward not only correctness but\nalsothequalityofintermediatereasoningsteps.Recentstudiessuggestthatwhile\nSFT “memorizes,” RL can help models “generalize” [11], offering a more stable\ntrajectory toward domain-transferable representations. Notably, GRPO [27] ex-\ntends PPO by eliminating its (neural) value function estimator and focusing on\na rule-based group-relative advantage for selecting actions, showing promise in\nresource-constrained settings like DeepSeek-R1 [12]. Such RL-driven frameworks\ncould be particularly beneficial for medical tasks, where limited data availability,\nhigh-stakes decision-making, and the need for explicit reasoning converge. In the\nfollowing section, we will detail how these insights motivate our approach.\n3 Methods\nOverview. We leverage RL to incentivize explicit reasoning capabilities in med-\nical VLMs, specifically employing GRPO due to its efficiency and effectiveness.4 Pan, Liu et al.\nQuestion Template\nWhat can be observed in this image?\nA) Cartilage degeneration B) Labral pathology\nC) Bone fracture D) Tendonitis\nModel Input Prompt\n{Question} Your task:\n1. Think through the question step by step, enclose your rea-\nsoning process in <think>...</think> tags.\n2. Then provide the correct single-letter choice (A, B, C, D,...) inside <an-\nswer>...</answer> tags.\n3. No extra information or text outside of these tags.\nModel Output Examples\n<think> The image is a grayscale MRI image of an upper arm joint. The bicondy-\nlar humeral head of the humerus is visible. There is a well-defined ... </think>\n<answer>B, there is no clear indication of ... </answer>\nFormat Reward = 1 due to the present of all tags and no content outside the tags\nAccuracy Reward = 0.5 due to extra explanation appended after the answer\nFig.1:The template of our employed prompt, an example of model’s response and\nreward criterion.\nWhiletheseminalwork[27]appliesGRPOtoreasoningincodingandmathemat-\nics, we adapt these principles to the medical domain, specifically radiology data\n(MRI, CT, X-ray). Our approach incorporates medical imaging prompts and\ncustom reward functions designed to encourage explicit reasoning and domain-\nspecific answer formats. This work serves as an initial exploration of using pure\nreinforcement learning to build a multi-modal medical reasoning model.\nBase Model and Prompt Template. We adopt a state-of-the-art VLM –\nQwen2-VL-2B [30] as our base model, denoted by πθ, where θare the train-\nable parameters. Given a training dataset V, each sample vconsists of: 1)\nAn image f, which is a radiology image and 2) a text prompt q, composed\nof the user’s question alongside a fixed system message, as illustrated in Fig-\nure 1. The VLM then produces an output {o}, which includes both a reasoning\ntrace and a final answer in designated XML-like tags ( <think>...</think> and\n<answer> ...</answer> ). Our RL objective is to optimize πθso that answers are\naccurate, well-formatted, and provide transparent reasoning.\nGroup Relative Policy Optimization (GRPO). To encourage robust, in-\nterpretableresponses,weemployGRPO[27],anRLalgorithmthatextendsPPO\nby focusing on a group-relative advantage instead of a learned value function.\nConcretely, at each training step:\n1. We sample Gcandidate outputs {oi}G\ni=1from πθold, the model parameters\nbefore the current update.MedVLM-R1 5\n2. We compute a reward rifor each output using a reward function (see next\nparagraph). Based on riwe calculate a group relative advantage Aiwhich\nis normalized by the group statistics: Ai=ri−mean({r1,r2,···,rG})\nstd({r1,r2,···,rG}).A reward\nabovethegroupaverageisadvantagedandcanfurtherincentivizethemodel.\n3. Our VLM model πθis then updated by maximizing JGRPOwhich incorpo-\nrates a clipped regularization on the relative advantage estimation for model\npreference alignment and training stability:\nJGRPO (θ) =Ev∼P(V)E{oi}G\ni=1∼πθold(·|v)\n1\nGGX\ni=1h\nmin\u0010\nrratio\niAi,clip\u0000\nrratio\ni,1±ϵ\u0001\nAi\u0011\n−βDKL(πθ||πref)i(3.1)\nwith rratio\ni=πθ(oi|v)\nπθold(oi|v). An additional Kullback–Leibler termDKL(πθ∥πref)\nis applied to penalize divergence from a reference model πref(the initial\ncheckpoint), helping prevent catastrophic forgetting. ϵ, β∈R≥0control\nthe regularization strengths.\nReward function. Specifically, for multiple-choice medical VQA tasks, we pro-\npose a two-part rule-based reward function, inspired by [12]:\n1)Format Reward. We incentivize outputs that provide a reasoning trace within\nthetags <think> ...</think> andasuccinctfinalanswerwithinthetag <answer>\n...</answer> . If all four tags are present exactly once and no content is present\noutside these tags, we assign a format reward of 1. Any missing/duplicated tags\nor content outside yield 0.\n2)Accuracy Reward. After verifying the correct format, we evaluate the correct-\nness of the final answer. Specifically, If the letter choice A, B, C, D, ...inside\nthe<answer> ...</answer> tag and it responds with the ground-truth choice\nexactly, that is an exact match with a reward of 1 point. Further, if the letter is\ncorrect but contains additional explanations or uses the choice content instead of\nthe corresponding letter (e.g., “A: Pulmonary nodule” or \"Pulmonary nodule\"),\nthat is a partial match with a 0.5 points reward. However, if the letter does not\nmatch, is missing, or the answer is not enclosed in the answer tag, that is an\nincorrect or missing answer and no reward would be granted (0 points).\nThe total reward ri∈[0,2]is the sum of both format and accuracy reward.\nBy structuring the reward function in this hierarchy (format before correctness),\nwe guide the model to first adopt the desired response structure, then refine its\nanswer selection for accurate, interpretable medical reasoning. It is worth noting\nthat both terms are necessary since without Format Reward , the final answer\ncannot be extracted while without Accuracy Reward , the model cannot converge.\n4 Experiments\nDataset. We conduct our experiments using the HuatuoGPT-Vision [7] evalu-\nation dataset, which is a processed and combined dataset from several publicly6 Pan, Liu et al.\navailable medical VQA benchmarks, including VQA-RAD [18], SLAKE [22],\nPathVQA [14], OmniMedVQA [15], and PMC-VQA [34]. In total, the dataset\ncomprises 17,300 multiple-choice questions linked to images covering various\nmedical imaging modalities, with 2–6 possible choices per question. For this\nstudy, we focus on radiology modalities: CT, MRI, and X-ray. Specifically, we\nuse 600 MRI image-question pairs for training and set aside 300 MRI, 300 CT,\nand 300 X-ray pairs for testing. The MRI test set is used as in-domain test,\nwhereas the CT and X-ray test set serve as OOD test.\nImplementation details. We adopt Qwen2-VL-2B as our base VLM. This\nmodelisoriginallytrainedondatafromcuratedwebpages,open-sourcedatasets,\nand synthetic sources. To adapt it to the medical domain, we employ the GRPO\nreinforcement learning framework outlined in Section 3. Our implementation\nbuilds on the public VLM reasoning repositories [23, 8, 28]. We perform fine-\ntuning on two NVIDIA A100 SXM4 80GB for 300 steps, using a batch size of 2,\nwhich takes approximately 4 hours. Generation candidate number Gis set to 6.\nThe other training optimization hyper-parameters are set as suggested by [8].\nBaseline methods and evaluation metric. We compare MedVLM-R1 with\nthe following baselines: 1. Qwen2-VL family [4] including Qwen2-VL-2B (the un-\nmodified base model), Qwen2-VL-7B and -72B which are the large/huge model\nvariants. 2. HuatuoGPT-vision [7]: A medical VLM built upon Qwen2-VL-7B. 3.\nSFT: The same Qwen2-VL-2B base model fine-tuned with standard SFT, using\nthesametrainingsettingwith600MRIquestion-answerpairs.Weapplynegative\nlog-likelihood as the loss function to carry out the SFT training. All baselines\nuse a simple prompting format, e.g., {Question} Your task: provide the\ncorrect single-letter choice (A, B, C, D, ...). In contrast, MedVLM-\nR1 uses the RL-based prompt as described in Section 3, designed to elicit explicit\nreasoning. For evaluation, each model receives one point for the correct single-\nletter answer and zero otherwise. In the test of MedVLM-R1, only the correct\nchoice enclosed in the <answer>...</answer> tag is scored as correct; any de-\nviation from this format, even if semantically correct, results in a zero score.\n5 Results and Discussion\nOverall Performance. Table 1 summarizes both in-domain (ID) and out-\nof-domain (OOD) performance for various VLMs. Note that ID/OOD com-\nparisons specifically refer to models fine-tuned on MRI data. Unsurprisingly,\nVLMs fine-tuned with both GRPO and SFT significantly outperform zero-shot\ngeneral-purposeVLMsonin-domaintasks.OurGRPO-trainedmodelshowsvery\nstrong OOD performance, achieving a 16%improvement on CT and a 35%\nimprovement on X-ray compared to SFT counterparts, underscoring GRPO’s\nsuperior generalizability. Furthermore, despite being a compact 2B-parameter\nmodel trained on just 600 samples, MedVLM-R1 outperforms larger models like\nQwen2-VL-72B and HuatuoGPT-Vision-7B, with the latter being specificallyMedVLM-R1 7\nProblem 1: What can be observed in this image?\nA) Lungs B) Bladder C) Brain D) Heart\nMedVLM-R1 Output:\n<think>\nThe image is a chest X-ray, which is a type of radiographic image used\nto visualize the internal structures of the body, particularly the lungs\nand bones. The presence of lung markings and the ribcage are charac-\nteristic features of a chest X-ray.\n</think>\n<answer>A</answer> Groundturth Answer: A\nProblem 2: What content appears in this image?\nA) Cardiac tissue, B) Breast tissue, C) Liver tissue, D) Skin tissue\nMedVLM-R1 Output:\n<think> The image appears to be a mammogram, which is a type of\nX-ray used to detect abnormalities in the breast tissue. The image\nshows the breast tissue with various densities and patterns, which are\ntypical of mammograms. </think>\n<answer>B</answer> Groundturth Answer: B\nProblem 3: What is the finding in this image?\nA) Stomach ulcer, B) Nerve compression,\nC) Cord pathology, D) Liver cirrhosis\nMedVLM-R1 Output:\n<think> To determine the correct answer, we need to identify the find-\ning in the image provided. The MRI scan in question is likely a view\nof a spine, not the stomach, or the liver. Stomach ulcers, liver cirrhosis,\nand spinal cord pathology can be diagnosed with MRI, but the image\nin question shows a spinal view, not a gastrointestinal or abdominal\nview. Therefore, the only reasonable options left are nerve compression and cord pathology.\n</think>\n<answer>C</answer> Groundturth Answer: C\nProblem 4: What is the specific abnormality present in this image?\nA) Gastritis, B) Arthritis, C) Appendicitis, D) Urolithiasis\nMedVLM-R1 Output:\n<think> The image presented is a transverse CT scan of the abdomen\nand pelvis. The presence of calculi (urines filled with stones or grit) in\nthe pelvic organs is a consistent finding in urolithiasis. </think>\n<answer>D</answer> Groundturth Answer: D\nFig.2:Medical VQA examples of MedVLM-R1 on X-ray (1, 2), MRI (3) and CT (4).\ntrained on large-scale medical data. This highlights the immense potential of\nRL-based training methods for efficient and scalable medical VLM development.\nReasoning Competence and Interpretability. Beyond strong generaliza-\ntion, a central strength of MedVLM-R1 is its ability to produce explicit reason-\ning—a capability absent in all baselines. As illustrated in Figure 2, MedVLM-R1\npresents a logical thought process within the <think> tag, with the final decision\nenclosed in the <answer> tag. Notably, for relatively simpler questions (prob-8 Pan, Liu et al.\nTable 1: Results of VQA-VLMs on MRI (in-domain), and CT and X-Ray (out-of-\ndomain) modalities. \" −2B\" indicates the model has 2billion parameters, etc.\nMethodNum. of Seen\nMedical SampleIn-Domain / Out-of-DomainAverage(MRI→MRI) (MRI →CT) (MRI →X-ray)\nRandom Guess / 25.00 30 .25 26 .00 27.08\nZero-shot VLM\nQwen2-VL-2B / 61.67 50 .67 53 .00 55.11\nQwen2-VL-7B / 72.33 68 .67 66 .63 69.21\nQwen2-VL-72B / 68.67 60 .67 72 .33 67.22\nZero-shot Medical VLM\nHuatuo-GPT-vision-7B 1,294,062 71.00 63 .00 73.66 69.22\nMRI fine-tuned VLM\nQwen2-VL-2B (SFT) 600 94.00 54 .33 34 .00 59.44\nOurs-2B (GRPO) 600 95.33 70 .33 69.00 78.22\nlem 1 and 2), the reasoning appears cogent and aligned with medical knowledge.\nHowever, more complex queries sometimes reveal heuristic or just partial reason-\ning. For example, in the third sample, the model arrives at the correct answer via\ntheprocess of elimination rather than detailed medical analysis, suggesting\nit leverages cue-based reasoning instead of domain expertise. Likewise, in some\ninstances (e.g., question 4), the causal chain between reasoning and conclusion\nremains unclear, raising the question of whether the model merely retrofits\nan explanation after predicting the correct answer . Despite these im-\nperfections, MedVLM-R1 represents a notable step toward interpretability in\nradiological decision-making.\nLimitations. Although MedVLM-R1 demonstrates promising results in MRI,\nCT, and X-ray datasets, several limitations remain: 1. Modality Gaps: When\ntested on other medical modalities (e.g., pathology or OCT images), the model\nfails to converge. We hypothesize this arises from the base model’s insufficient\nexposure to such modalities during pre-training. 2. Closed-Set Dependence: The\ncurrent approach is tailored to multiple-choice (closed-set) VQA. In open-ended\nquestion settings where no predefined options are provided, the model’s perfor-\nmance degrades substantially. This is also a common challenge for many VLMs.\n3. Superficial/hallucinated Reasoning: In some reasoning cases, MedVLM-R1\nprovides a correct answer without offering a meaningful reasoning process (e.g.,\n<think>To determine the correct observation from this spine MRI, let’s\nanalyze the image.</think><answer>B</answer> ).Moreover,sometimesthe\nmodel concludes a correct choice while providing an inference that can lead to\nanother answer. This phenomenon underscores that even models designed for\nexplainability can occasionally revert to superficial/hallucinated justifications,\nhighlighting an ongoing challenge in generating consistently transparent and\nlogically sound rationales. Regarding all these issues, we believe the current 2B-\nparameterscaleofourbasemodelconstitutesapotentialbottleneck,andweplan\nto evaluate MedVLM-R1 on larger VLM backbones to address these concerns.MedVLM-R1 9\n6 Conclusion\nWepresentMedVLM-R1,amedicalVLMthatintegratesGRPO-basedreinforce-\nment learning to bridge the gap between accuracy, interpretability, and robust\nperformance in radiology VQA. By focusing on explicit reasoning, the model\nfosters transparency and trustworthiness—qualities essential in high-stakes clin-\nical environments. Our results demonstrate that RL-based approaches generalize\nbetter than purely SFT methods, particularly under OOD settings. Although\nVLM-based medical reasoning is still at a nascent stage and faces considerable\nchallenges, we believe that its potential for delivering safer, more transparent\nAI-driven healthcare solutions will be appreciated and should be encouraged.\n7 Acknowledgements\nThis work is partially funded by the European Research Council (ERC) project\nDeep4MI (884622). Mr. Wu is supported by the Engineering and Physical Sci-\nences Research Council (EPSRC) under grant EP/S024093/1 and GE Health-\nCare. Mr. Liu is supported by the Clarendon Fund. Ms. Zhu is supported by\nthe Engineering and Physical Sciences Research Council (EPSRC) under grant\nEP/S024093/1 and Global Health R&D of the healthcare business of Merck\nKGaA, Darmstadt, Germany, Ares Trading S.A. (an affiliate of Merck KGaA,\nDarmstadt, Germany), Eysins, Switzerland (Crossref Funder ID: 10.13039 /\n100009945). Dr. Li is supported by a Postdoc Mobility Grant from SNSF. Dr.\nChen is funded by Royal Society (RGS/R2/242355). Dr. Ouyang is supported\nby UKRI grant EP/X040186/1.\nReferences\n1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,\nD., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774 (2023)\n2. Akhter, Y., Singh, R., Vatsa, M.: Ai-based radiodiagnosis using chest x-rays: A\nreview. Frontiers in big data 6, 1120989 (2023)\n3. Anthropic: Claude 3.7 sonnet system card (2025)\n4. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.:\nQwen-vl: A versatile vision-language model for understanding, localization, text\nreading, and beyond. arXiv preprint arXiv:2308.12966 (2023)\n5. Chaves, J.M.Z., Huang, S.C., Xu, Y., Xu, H., Usuyama, N., Zhang, S., Wang, F.,\nXie, Y., Khademi, M., Yang, Z., et al.: Towards a clinically accessible radiology\nfoundation model: open-access and lightweight, with automated evaluation. arXiv\npreprint arXiv:2403.08002 (2024)\n6. Chen, J., Yang, D., Jiang, Y., Li, M., Wei, J., Hou, X., Zhang, L.: Efficiency in\nfocus: Layernorm as a catalyst for fine-tuning medical visual language pre-trained\nmodels. arXiv preprint arXiv:2404.16385 (2024)\n7. Chen, J., Ouyang, R., Gao, A., Chen, S., Chen, G.H., Wang, X., Zhang, R., Cai,\nZ., Ji, K., Yu, G., Wan, X., Wang, B.: Huatuogpt-vision, towards injecting medical\nvisual knowledge into multimodal llms at scale (2024)10 Pan, Liu et al.\n8. Chen, L., Li, L., Zhao, H., Song, Y., Vinci: R1-v: Reinforcing super generaliza-\ntion ability in vision-language models with less than $3. https://github.com/\nDeep-Agent/R1-V (2025), accessed: 2025-02-02\n9. Chen, W., Ma, X., Wang, X., Cohen, W.W.: Program of thoughts prompting:\nDisentangling computation from reasoning for numerical reasoning tasks. arXiv\npreprint arXiv:2211.12588 (2022)\n10. Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S., Amodei, D.: Deep\nreinforcement learning from human preferences. Advances in neural information\nprocessing systems 30(2017)\n11. Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q.V., Levine,\nS., Ma, Y.: Sft memorizes, rl generalizes: A comparative study of foundation model\npost-training. arXiv preprint arXiv:2501.17161 (2025)\n12. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S.,\nWang, P., Bi, X., et al.: Deepseek-r1: Incentivizing reasoning capability in llms via\nreinforcement learning. arXiv preprint arXiv:2501.12948 (2025)\n13. Hartsock, I., Rasool, G.: Vision-language models for medical report generation and\nvisual question answering: A review. Frontiers in Artificial Intelligence 7(2024)\n14. He, X., Zhang, Y., Mou, L., Xing, E., Xie, P.: Pathvqa: 30000+ questions for\nmedical visual question answering. arXiv preprint arXiv:2003.10286 (2020)\n15. Hu, Y., Li, T., Lu, Q., Shao, W., He, J., Qiao, Y., Luo, P.: Omnimedvqa: A new\nlarge-scale comprehensive evaluation benchmark for medical lvlm. In: Conference\non Computer Vision and Pattern Recognition. pp. 22170–22183 (2024)\n16. Hurst, A., Lerer, A., Goucher, A.P., Perelman, A., Ramesh, A., Clark, A., Os-\ntrow, A., Welihinda, A., Hayes, A., Radford, A., et al.: Gpt-4o system card. arXiv\npreprint arXiv:2410.21276 (2024)\n17. Kumar,A.,Zhuang,V.,Agarwal,R.,Su,Y.,Co-Reyes,J.D.,Singh,A.,Baumli,K.,\nIqbal, S., Bishop, C., Roelofs, R., et al.: Training language models to self-correct\nvia reinforcement learning. arXiv preprint arXiv:2409.12917 (2024)\n18. Lau, J.J., Gayen, S., Ben Abacha, A., et al.: A dataset of clinically generated visual\nquestions and answers about radiology images. Scientific data 5(1), 1–10 (2018)\n19. Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T.,\nPoon, H., Gao, J.: Llava-med: Training a large language-and-vision assistant for\nbiomedicine in one day. Advances in Neural Information Processing Systems 36,\n28541–28564 (2023)\n20. Li, L.H., Hessel, J., Yu, Y., Ren, X., Chang, K.W., Choi, Y.: Symbolic chain-of-\nthought distillation: Small models can also\" think\" step-by-step. arXiv preprint\narXiv:2306.14050 (2023)\n21. Lian, C., Zhou, H.Y., Yu, Y., Wang, L.: Less could be better: Parameter-\nefficient fine-tuning advances medical vision foundation models. arXiv preprint\narXiv:2401.12215 (2024)\n22. Liu, B., Zhan, L.M., Xu, L., Ma, L., Yang, Y., Wu, X.M.: Slake: A semantically-\nlabeled knowledge-enhanced dataset for medical visual question answering. In: in-\nternational symposium on biomedical imaging (ISBI). pp. 1650–1654 (2021)\n23. LMMs-Lab: open-r1-multimodal. https://github.com/EvolvingLMMs-Lab/\nopen-r1-multimodal (2025), accessed: 2025-01-27\n24. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,\nC., Agarwal, S., Slama, K., Ray, A., et al.: Training language models to follow\ninstructions with human feedback. Advances in neural information processing sys-\ntems35, 27730–27744 (2022)\n25. Qwen-Team: Qwq: Reflect deeply on the boundaries of the unknown (2024)MedVLM-R1 11\n26. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347 (2017)\n27. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li,\nY., Wu, Y., et al.: Deepseekmath: Pushing the limits of mathematical reasoning in\nopen language models. arXiv preprint arXiv:2402.03300 (2024)\n28. Shen, H., Zhang, Z., Zhang, Q., Xu, R., Zhao, T.: Vlm-r1: A stable and gen-\neralizable r1-style large vision-language model. https://github.com/om-ai-lab/\nVLM-R1(2025), accessed: 2025-02-15\n29. Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A.,\nHubert, T., Baker, L., Lai, M., Bolton, A., et al.: Mastering the game of go without\nhuman knowledge. nature 550(7676), 354–359 (2017)\n30. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang,\nJ., Ge, W., et al.: Qwen2-vl: Enhancing vision-language model’s perception of the\nworld at any resolution. arXiv preprint arXiv:2409.12191 (2024)\n31. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,\nD., et al.: Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in neural information processing systems 35, 24824–24837 (2022)\n32. Wu, C., Zhang, X., Zhang, Y., Wang, Y., Xie, W.: Towards generalist foundation\nmodel for radiology by leveraging web-scale 2d&3d medical data. arXiv preprint\narXiv:2308.02463 (2023)\n33. Zhang, K., Zhou, R., Adhikarla, E., Yan, Z., Liu, Y., Yu, J., Liu, Z., Chen, X.,\nDavison, B.D., Ren, H., et al.: A generalist vision–language foundation model for\ndiverse biomedical tasks. Nature Medicine pp. 1–13 (2024)\n34. Zhang, X., Wu, C., Zhao, Z., Lin, W., Zhang, Y., Wang, Y., Xie, W.: Pmc-vqa:\nVisual instruction tuning for medical visual question answering. arXiv preprint\narXiv:2305.10415 (2023)\n35. Ziegler, D.M., Stiennon, N., Wu, J., Brown, T.B., Radford, A., Amodei, D., Chris-\ntiano, P., Irving, G.: Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593 (2019)",
    "result": {
      "result": "### Key Findings\n\n1. **Introduction of MedVLM-R1**:\n   - **Purpose**: MedVLM-R1 is a medical Vision-Language Model (VLM) designed to generate explicit natural language reasoning to enhance transparency and trustworthiness in medical image analysis.\n   - **Method**: Utilizes a reinforcement learning (RL) framework instead of supervised fine-tuning (SFT) to foster genuine reasoning capabilities.\n\n2. **Performance and Efficiency**:\n   - **Accuracy Improvement**: Despite limited training data (600 visual question answering samples) and model parameters (2B), MedVLM-R1 boosts accuracy from 55.11% to 78.22% across MRI, CT, and X-ray benchmarks, outperforming larger models trained on over a million samples.\n   - **Generalization**: Demonstrates robust domain generalization under out-of-distribution tasks, indicating superior performance in varied clinical scenarios.\n\n3. **Reinforcement Learning Advantages**:\n   - **RL vs SFT**: RL incentivizes models to discover human-interpretable reasoning paths, whereas SFT often leads to overfitting and limited generalization.\n   - **GRPO**: Utilizes Group Relative Policy Optimization (GRPO), which eliminates the need for neural reward models, reducing computational demands and enhancing reasoning capabilities.\n\n4. **Key Contributions**:\n   - **Explicit Reasoning**: MedVLM-R1 generates explicit reasoning alongside final answers, enhancing interpretability.\n   - **Emergent Reasoning Without Supervision**: Trained using GRPO with datasets containing only final answers, demonstrating emergent reasoning capabilities without explicit supervision.\n   - **Superior Generalization**: Achieves robust generalization to out-of-distribution data, outperforming larger models like Qwen2VL-72B and Huatuo-GPT-Vision-7B.\n\n5. **Experimental Results**:\n   - **Dataset**: Utilizes the HuatuoGPT-Vision evaluation dataset, comprising 17,300 multiple-choice questions linked to images covering various medical imaging modalities.\n   - **Evaluation**: MedVLM-R1 shows significant improvements in out-of-domain (OOD) performance, achieving a 16% improvement on CT and a 35% improvement on X-ray compared to SFT counterparts.\n\n6. **Reasoning Competence and Interpretability**:\n   - **Logical Thought Process**: MedVLM-R1 presents a logical thought process within the <think> tag, with the final decision enclosed in the <answer> tag, enhancing interpretability.\n   - **Limitations**: Occasionally provides heuristic or partial reasoning, highlighting the need for further refinement in generating consistently transparent and logically sound rationales.\n\n7. **Limitations and Future Work**:\n   - **Modality Gaps**: Fails to converge on other medical modalities like pathology or OCT images.\n   - **Closed-Set Dependence**: Tailored to multiple-choice (closed-set) VQA, with degraded performance in open-ended question settings.\n   - **Superficial/Hallucinated Reasoning**: Sometimes provides correct answers without meaningful reasoning, indicating occasional superficial/hallucinated justifications.\n\n8. **Conclusion**:\n   - **Potential**: MedVLM-R1 represents a significant step toward trustworthy and interpretable AI in clinical practice, fostering transparency and trustworthiness essential in high-stakes clinical environments.\n   - **Future Directions**: Encourages further exploration and development of RL-based training methods for efficient and scalable medical VLM development.\n\nThese findings underscore the potential of RL-based approaches in enhancing the interpretability and generalization of medical VLMs, paving the way for more transparent and trustworthy AI-driven healthcare solutions."
    },
    "processed_by": "mistral"
  },
  {
    "source": "direct",
    "query": "Poker",
    "task": "text_classification",
    "prompt": "Classify the topic of this text",
    "content": "Poker",
    "result": {
      "result": "Based on the provided text \"Poker,\" the topic can be classified as:\n\n**Games & Recreation > Card Games**\n\nMore specifically, it falls under the category of:\n\n**Card Games > Casino Games > Poker**\n\nHere's a breakdown:\n- **Games & Recreation**: The broad category for various types of games and recreational activities.\n- **Card Games**: A subcategory for games played with cards.\n- **Casino Games**: A further subcategory for card games commonly played in casinos.\n- **Poker**: The specific game mentioned in the text."
    },
    "processed_by": "mistral"
  }
]